{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import vstack\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from numpy import savez_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path, size=(256,512)):\n",
    "    source_imgs_list, target_imgs_list = list(), list()\n",
    "    for filename in listdir(path):\n",
    "        img_pixels = load_img(path + filename, target_size=size)\n",
    "        img_pixels = img_to_array(img_pixels)\n",
    "        # The source and target images came as a unique image where half of it is\n",
    "        # the source and the other half the target.\n",
    "        source_img, target_img = img_pixels[:, :256], img_pixels[:, 256:]\n",
    "        source_imgs_list.append(source_img)\n",
    "        target_imgs_list.append(target_img)\n",
    "    return [asarray(source_imgs_list), asarray(target_imgs_list)]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = \"./maps/maps/train/\"\n",
    "if not Path(\"./maps_256.npz\").exists():\n",
    "    [source_images, target_images] = load_images(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'maps_256.npz'\n",
    "if not Path(\"./maps_256.npz\").exists():\n",
    "    savez_compressed(filename, source_images, target_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCRIMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture we're using for the discriminator is the *U-Net*. The paper clearly states how we should build the discriminator:\n",
    " \n",
    "> For  discriminator  networks,  we  use 70×7 Patch-GAN   [21].Let Ck denote a **4×4 Convolution-InstanceNorm-LeakyReLU layer** with **k filters** and **stride 2**.After the last layer, we apply a convolution to produce a 1 dimensional output.  We do not use InstanceNorm for the first C64 layer.  We use **leaky ReLUs with slope 0.2.**  All our three discriminators have the identical architecture as follows:C64-C128-C256-C51\n",
    "\n",
    "Also it says this *for every network* :\n",
    "\n",
    "> All  the  networks  were  trained  from  scratch,  using  the **Adam solver [27] and a learning rate of 0.0002**.  We keep the same learning rate for the first 100 epochs and linearly decay the rate to zero over the next 100 epochs.  **Weights were initialized from a Gaussian distribution with mean 0 and standard deviation 0.02.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we concieve a \"block\" in this context as the set of operations that are executed repeatedly, \n",
    "# then a discriminator block only changes in the number of filters it has, it stride length and \n",
    "# the input that it receives. Also, in the first layer the batch normalization musn't be performed.\n",
    "\n",
    "def discriminator_block(init, num_filters, stride_length, input, apply_batch_norm = True):\n",
    "    \n",
    "    output = Conv2D(num_filters, (4,4), strides=stride_length, padding='same', kernel_initializer=init)(input)\n",
    "    if(apply_batch_norm):\n",
    "        output = BatchNormalization()(output)\n",
    "    output = LeakyReLU(alpha=0.2)(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![U-Net Photo](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Architecture-of-the-U-Net-Generator-Model-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As Input to the discriminator a source image and its target image are passed and then concatenated.\n",
    "# The target image is the one we're trying to generate so it can either be a real or a fake one.\n",
    "# The source image will remain always the same and condition the discriminator network.\n",
    "\n",
    "def discriminator(image_shape):\n",
    "    \n",
    "    kernel_initializer = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    input_source_image = Input(shape=image_shape)\n",
    "    input_target_image = Input(shape=image_shape)\n",
    "    \n",
    "    merged_inputs = Concatenate()([input_source_image, input_target_image])\n",
    "    \n",
    "    dis_output = discriminator_block(kernel_initializer, 64,  (2,2), merged_inputs, False)\n",
    "    dis_output = discriminator_block(kernel_initializer, 128, (2,2), dis_output)\n",
    "    dis_output = discriminator_block(kernel_initializer, 256, (2,2), dis_output)\n",
    "    dis_output = discriminator_block(kernel_initializer, 512, (2,2), dis_output)\n",
    "    dis_output = discriminator_block(kernel_initializer, 512, (1,1), dis_output)\n",
    "    \n",
    "    # Here we're using as a final output activation layer a Sigmoid function because we want to generate\n",
    "    # an image of patches where each patch is a number between 0 or 1 representing the likelihood of that \n",
    "    # patch being real or fake (generated).\n",
    "    dis_output = Conv2D(1, (4,4), padding='same', kernel_initializer=kernel_initializer)(dis_output)\n",
    "    patch_out = Activation('sigmoid')(dis_output)\n",
    "    \n",
    "    model = Model([input_source_image, input_target_image], patch_out)\n",
    "    \n",
    "    optimizer_technique = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer_technique, loss_weights=[0.5])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PatchGAN](https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix/images/unet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(input, num_filters, apply_batch_norm=True):\n",
    "    \n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    enc_output = Conv2D(num_filters, (4,4), strides=(2,2), padding=\"same\", kernel_initializer=init)(input)\n",
    "    if(apply_batch_norm):\n",
    "        enc_output = BatchNormalization()(enc_output, training=True)\n",
    "    enc_output = LeakyReLU(alpha=0.2)(enc_output)\n",
    "\n",
    "    return enc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder blocks take as input among others the skip connections with the encoders. This skip connections\n",
    "# are then merged with the output of the normalized deconvoluted input.\n",
    "def decoder_block(input, skip_connection, num_filters, apply_dropout=True):\n",
    "    \n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    dec_output = Conv2DTranspose(num_filters, (4,4), strides=(2,2), padding=\"same\", kernel_initializer=init)(input)\n",
    "    dec_output = BatchNormalization()(dec_output, training=True)\n",
    "    if(apply_dropout):\n",
    "        dec_output = Dropout(0.5)(dec_output, training=True)\n",
    "    dec_output = Concatenate()([dec_output, skip_connection])\n",
    "    dec_output = Activation('relu')(dec_output)\n",
    "    \n",
    "\n",
    "    return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator takes as input an image, this image will be the source image or (in our case) a semantic\n",
    "# labeling map from which we want to obtain a realistic image (target image).\n",
    "def generator(image_shape=(256,256,3)):\n",
    "    \n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    input_image = Input(shape=image_shape)\n",
    "    \n",
    "    encoder_output_1 = encoder_block(input_image, 64, apply_batch_norm=False)\n",
    "    encoder_output_2 = encoder_block(encoder_output_1 , 128)\n",
    "    encoder_output_3 = encoder_block(encoder_output_2, 256)\n",
    "    encoder_output_4 = encoder_block(encoder_output_3, 512)\n",
    "    encoder_output_5 = encoder_block(encoder_output_4, 512)\n",
    "    encoder_output_6 = encoder_block(encoder_output_5, 512)\n",
    "    encoder_output_7 = encoder_block(encoder_output_6, 512)\n",
    "    \n",
    "    bottleneck = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(encoder_output_7)\n",
    "    bottleneck = Activation('relu')(bottleneck)\n",
    "    \n",
    "    decoder_output_1 = decoder_block(bottleneck, encoder_output_7, 512)\n",
    "    decoder_output_2 = decoder_block(decoder_output_1, encoder_output_6, 512)\n",
    "    decoder_output_3 = decoder_block(decoder_output_2, encoder_output_5, 512)\n",
    "    decoder_output_4 = decoder_block(decoder_output_3, encoder_output_4, 512)\n",
    "    decoder_output_5 = decoder_block(decoder_output_4, encoder_output_3, 256, apply_dropout=False)\n",
    "    decoder_output_6 = decoder_block(decoder_output_5, encoder_output_2, 128, apply_dropout=False)\n",
    "    decoder_output_7 = decoder_block(decoder_output_6, encoder_output_1, 64,  apply_dropout=False)\n",
    "    \n",
    "    generator_output = Conv2DTranspose(3, (4,4), strides=(2,2), padding=\"same\", kernel_initializer=init)(decoder_output_7)\n",
    "    output_image = Activation('tanh')(generator_output)\n",
    "    \n",
    "    model = Model(input_image, output_image)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan(generator, discriminator, image_shape):\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    input_source_img = Input(shape=image_shape)\n",
    "    \n",
    "    generator_output = generator(input_source_img)\n",
    "    discriminator_output = discriminator([input_source_img, generator_output])\n",
    "    \n",
    "    model = Model(input_source_img, [discriminator_output, generator_output])\n",
    "    optimizer_technique = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=optimizer_technique, loss_weights=[1,100])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples(filename):\n",
    "    data = load(filename)\n",
    "    source_imgs , target_imgs = data['arr_0'], data['arr_1']\n",
    "    source_imgs = (source_imgs - 127.5) / 127.5\n",
    "    target_imgs = (target_imgs - 127.5) / 127.5\n",
    "    return [source_imgs, target_imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_real_samples(dataset, n_samples, patch_shape):\n",
    "    source_imgs, target_imgs = dataset\n",
    "    random_index = randint(0, source_imgs.shape[0], n_samples)\n",
    "    source_imgs, target_imgs = source_imgs[random_index], target_imgs[random_index]\n",
    "    real_img_labels = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return [source_imgs, target_imgs], real_img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, samples, patch_shape):\n",
    "    fake_instances = generator.predict(samples)\n",
    "    fake_img_labels = zeros((len(fake_instances), patch_shape, patch_shape, 1))\n",
    "    return fake_instances, fake_img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_img_pixels(img):\n",
    "    return (img + 1) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Separate functionality in this function, you're not just summarizing the performance\n",
    "# but also saving the model. Split that.\n",
    "def summarize_performance(step, generator, dataset, n_samples=3):\n",
    "    \n",
    "    [source_imgs, target_imgs], _ = retrieve_real_samples(dataset, n_samples, 1)\n",
    "    fake_imgs, _ = generate_fake_samples(generator, source_imgs, 1)\n",
    "    \n",
    "    source_imgs = scale_img_pixels(source_imgs)\n",
    "    target_imgs = scale_img_pixels(target_imgs)\n",
    "    fake_imgs = scale_img_pixels(fake_imgs)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(source_imgs[i])\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(fake_imgs[i])\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(target_imgs[i])\n",
    "    \n",
    "    filename1 = 'plot_%06d.png' % (step + 1)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "    \n",
    "    # Yo quitaría esto de aquí\n",
    "    filename2 = 'model_%06d.h5' % (step + 1)\n",
    "    generator.save(filename2)\n",
    "    print('Saved: %s and %s' % (filename1, filename2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(discriminator, generator, gan, dataset, n_epochs=100, n_batch=1):\n",
    "    \n",
    "    discriminator_patch_shape = discriminator.output_shape[1]\n",
    "    source_imgs, target_imgs = dataset\n",
    "    iterations = int(len(source_imgs) / n_batch)\n",
    "    n_steps = iterations * n_epochs\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        [source_imgs, target_imgs], real_labels = retrieve_real_samples(dataset, n_batch, discriminator_patch_shape)\n",
    "        fake_imgs, fake_labels = generate_fake_samples(generator, source_imgs, discriminator_patch_shape)\n",
    "        disc_loss_real = discriminator.train_on_batch([source_imgs, target_imgs], real_labels)\n",
    "        disc_loss_fake = discriminator.train_on_batch([source_imgs, fake_imgs], fake_labels)\n",
    "        gan_loss, _, _ = gan.train_on_batch(source_imgs, [real_labels, target_imgs])\n",
    "        \n",
    "        print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, disc_loss_real, disc_loss_fake, gan_loss))\n",
    "        if (i % 10) == 0:\n",
    "            summarize_performance(i, generator, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_real_samples('maps_256.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_shape = dataset[0].shape[1:]\n",
    "image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "discriminator = discriminator(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = gan(generator, discriminator, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: plot_000002.png and model_000002.h5\n"
     ]
    }
   ],
   "source": [
    "summarize_performance(1, generator, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, d1[0.244] d2[0.982] g[78.605]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: plot_000001.png and model_000001.h5\n",
      ">2, d1[0.208] d2[0.702] g[67.284]\n",
      ">3, d1[0.206] d2[0.606] g[61.548]\n",
      ">4, d1[0.357] d2[0.421] g[74.253]\n",
      ">5, d1[0.364] d2[0.439] g[72.081]\n",
      ">6, d1[0.313] d2[0.409] g[68.554]\n",
      ">7, d1[0.219] d2[0.358] g[65.199]\n",
      ">8, d1[0.086] d2[0.233] g[49.261]\n",
      ">9, d1[0.320] d2[0.232] g[61.505]\n",
      ">10, d1[0.394] d2[0.256] g[52.505]\n",
      ">11, d1[0.107] d2[0.247] g[56.560]\n",
      "Saved: plot_000011.png and model_000011.h5\n",
      ">12, d1[0.045] d2[0.284] g[53.953]\n",
      ">13, d1[0.091] d2[0.132] g[52.444]\n",
      ">14, d1[0.245] d2[0.328] g[29.835]\n",
      ">15, d1[0.118] d2[0.105] g[40.900]\n",
      ">16, d1[0.172] d2[0.149] g[31.991]\n",
      ">17, d1[0.171] d2[0.066] g[27.440]\n",
      ">18, d1[0.274] d2[0.153] g[33.529]\n",
      ">19, d1[0.028] d2[0.069] g[41.253]\n",
      ">20, d1[0.439] d2[0.245] g[26.894]\n",
      ">21, d1[0.051] d2[0.024] g[33.966]\n",
      "Saved: plot_000021.png and model_000021.h5\n",
      ">22, d1[0.017] d2[0.016] g[36.236]\n",
      ">23, d1[0.015] d2[0.052] g[35.805]\n",
      ">24, d1[0.062] d2[0.062] g[31.890]\n",
      ">25, d1[0.009] d2[0.060] g[29.056]\n",
      ">26, d1[0.049] d2[0.052] g[29.113]\n",
      ">27, d1[0.041] d2[0.065] g[27.345]\n",
      ">28, d1[0.034] d2[0.050] g[26.936]\n",
      ">29, d1[0.124] d2[0.855] g[25.169]\n",
      ">30, d1[2.376] d2[0.025] g[21.756]\n",
      ">31, d1[1.246] d2[0.210] g[19.376]\n",
      "Saved: plot_000031.png and model_000031.h5\n",
      ">32, d1[0.338] d2[0.468] g[19.417]\n",
      ">33, d1[0.766] d2[0.861] g[20.983]\n",
      ">34, d1[0.678] d2[0.629] g[17.134]\n",
      ">35, d1[0.288] d2[0.354] g[20.526]\n",
      ">36, d1[0.257] d2[0.441] g[22.187]\n",
      ">37, d1[0.707] d2[0.245] g[16.884]\n",
      ">38, d1[0.590] d2[0.424] g[18.808]\n",
      ">39, d1[0.271] d2[0.388] g[15.572]\n",
      ">40, d1[0.200] d2[0.310] g[16.272]\n",
      ">41, d1[0.587] d2[0.340] g[22.014]\n",
      "Saved: plot_000041.png and model_000041.h5\n",
      ">42, d1[0.733] d2[0.299] g[12.143]\n",
      ">43, d1[0.124] d2[0.409] g[16.520]\n",
      ">44, d1[0.530] d2[0.231] g[30.149]\n",
      ">45, d1[0.291] d2[0.423] g[19.587]\n",
      ">46, d1[0.824] d2[0.196] g[12.837]\n",
      ">47, d1[0.290] d2[0.274] g[17.136]\n",
      ">48, d1[0.548] d2[0.455] g[12.492]\n",
      ">49, d1[0.359] d2[0.264] g[17.079]\n",
      ">50, d1[0.362] d2[0.174] g[13.595]\n",
      ">51, d1[0.672] d2[0.561] g[9.270]\n",
      "Saved: plot_000051.png and model_000051.h5\n",
      ">52, d1[0.638] d2[0.385] g[9.073]\n",
      ">53, d1[0.372] d2[0.383] g[10.161]\n",
      ">54, d1[0.250] d2[0.284] g[11.729]\n",
      ">55, d1[0.119] d2[0.223] g[23.595]\n",
      ">56, d1[0.777] d2[0.169] g[11.482]\n",
      ">57, d1[0.330] d2[0.177] g[12.889]\n",
      ">58, d1[0.228] d2[0.145] g[27.528]\n",
      ">59, d1[0.214] d2[0.552] g[11.709]\n",
      ">60, d1[0.164] d2[0.075] g[20.757]\n",
      ">61, d1[0.404] d2[0.037] g[32.700]\n",
      "Saved: plot_000061.png and model_000061.h5\n",
      ">62, d1[0.569] d2[0.201] g[13.445]\n",
      ">63, d1[0.469] d2[0.800] g[10.239]\n",
      ">64, d1[0.206] d2[0.405] g[13.090]\n",
      ">65, d1[0.099] d2[0.172] g[15.113]\n",
      ">66, d1[0.284] d2[0.108] g[10.264]\n",
      ">67, d1[0.859] d2[0.350] g[9.425]\n",
      ">68, d1[0.717] d2[0.347] g[7.578]\n",
      ">69, d1[0.133] d2[0.164] g[16.325]\n",
      ">70, d1[0.376] d2[0.173] g[27.731]\n",
      ">71, d1[0.195] d2[0.290] g[9.491]\n",
      "Saved: plot_000071.png and model_000071.h5\n",
      ">72, d1[0.194] d2[0.501] g[9.913]\n",
      ">73, d1[0.116] d2[0.043] g[24.550]\n",
      ">74, d1[0.092] d2[0.060] g[12.623]\n",
      ">75, d1[0.132] d2[0.051] g[25.154]\n",
      ">76, d1[0.198] d2[0.048] g[17.912]\n",
      ">77, d1[0.141] d2[0.040] g[24.506]\n",
      ">78, d1[0.067] d2[0.128] g[18.334]\n",
      ">79, d1[0.975] d2[0.747] g[8.324]\n",
      ">80, d1[0.061] d2[0.400] g[10.954]\n",
      ">81, d1[0.094] d2[0.132] g[17.111]\n",
      "Saved: plot_000081.png and model_000081.h5\n",
      ">82, d1[0.524] d2[0.165] g[9.560]\n",
      ">83, d1[0.054] d2[0.222] g[20.965]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-ee7d95511da5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-8f4ef3436cec>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(discriminator, generator, gan, dataset, n_epochs, n_batch)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdisc_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_imgs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdisc_loss_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_imgs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mgan_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreal_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_imgs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'>%d, d1[%.3f] d2[%.3f] g[%.3f]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_loss_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc_loss_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(discriminator, generator, gan, dataset, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
